#!/usr/bin/env python3
"""
Tokamak AI CLI Tool
ëª…ë ¹ì¤„ì—ì„œ ë¹ ë¥´ê²Œ AIì™€ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” ë„êµ¬
"""

import os
import sys
from openai import OpenAI
from dotenv import load_dotenv
import argparse

load_dotenv()

class TokamakAI:
    def __init__(self, model=None):
        self.api_key = os.getenv("AI_API_KEY")
        self.base_url = os.getenv("AI_BASE_URL")
        self.model = model or os.getenv("AI_MODEL")
        
        if not self.api_key:
            raise ValueError("AI_API_KEY must be set in .env file")
        if not self.base_url:
            raise ValueError("AI_BASE_URL must be set in .env file")
            
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
    
    def ask(self, question, stream=True):
        """ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€"""
        try:
            if stream:
                return self._stream_response(question)
            else:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[{"role": "user", "content": question}]
                )
                return response.choices[0].message.content
        except Exception as e:
            return f"Error: {e}"
    
    def _stream_response(self, question):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": question}],
            stream=True
        )
        
        full_response = ""
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        print()  # ì¤„ë°”ê¿ˆ
        return full_response
    
    def chat(self):
        """ëŒ€í™”í˜• ëª¨ë“œ"""
        print(f"ğŸ¤– Tokamak AI Chat (Model: {self.model})")
        print("Type 'exit' or 'quit' to end the conversation.\n")
        
        messages = []
        
        while True:
            try:
                user_input = input("You: ").strip()
                
                if user_input.lower() in ['exit', 'quit', 'q']:
                    print("ğŸ‘‹ Goodbye!")
                    break
                
                if not user_input:
                    continue
                
                messages.append({"role": "user", "content": user_input})
                
                print("AI: ", end="", flush=True)
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    stream=True
                )
                
                full_response = ""
                for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        print(content, end="", flush=True)
                        full_response += content
                
                print("\n")
                messages.append({"role": "assistant", "content": full_response})
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}\n")

def main():
    parser = argparse.ArgumentParser(
        description="Tokamak AI CLI Tool - AIì™€ ëª…ë ¹ì¤„ì—ì„œ ëŒ€í™”í•˜ê¸°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì œ:
  # ë‹¨ì¼ ì§ˆë¬¸
  python ai.py "Pythonìœ¼ë¡œ ì›¹ í¬ë¡¤ëŸ¬ ë§Œë“œëŠ” ë°©ë²•"
  
  # ëŒ€í™”í˜• ëª¨ë“œ
  python ai.py --chat
  
  # ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©
  python ai.py --model qwen3-80b-next "ë¸”ë¡ì²´ì¸ì´ë€?"
  
  # ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì „ì²´ ì‘ë‹µ ë°›ê¸°
  python ai.py --no-stream "ê°„ë‹¨í•œ ì‹œ í•˜ë‚˜ ì¨ì¤˜"
        """
    )
    
    parser.add_argument(
        'question',
        nargs='?',
        help='AIì—ê²Œ ë¬¼ì–´ë³¼ ì§ˆë¬¸'
    )
    
    parser.add_argument(
        '-c', '--chat',
        action='store_true',
        help='ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘'
    )
    
    parser.add_argument(
        '-m', '--model',
        default=None,
        help='ì‚¬ìš©í•  AI ëª¨ë¸ (ê¸°ë³¸ê°’: qwen3-235b)'
    )
    
    parser.add_argument(
        '--no-stream',
        action='store_true',
        help='ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ ì „ì²´ ì‘ë‹µ ë°›ê¸°'
    )
    
    parser.add_argument(
        '--list-models',
        action='store_true',
        help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ'
    )
    
    args = parser.parse_args()
    
    try:
        ai = TokamakAI(model=args.model)
        
        if args.list_models:
            print("ğŸ“‹ Available models:")
            try:
                models = ai.client.models.list()
                for model in models.data:
                    print(f"  - {model.id}")
            except Exception as e:
                print(f"Error fetching models: {e}")
                print("  - qwen3-235b (default)")
                print("  - qwen3-80b-next")
            return
        
        if args.chat:
            ai.chat()
        elif args.question:
            print(f"ğŸ¤” Question: {args.question}\n")
            print("ğŸ¤– AI: ", end="" if not args.no_stream else "\n", flush=True)
            response = ai.ask(args.question, stream=not args.no_stream)
            if args.no_stream:
                print(response)
        else:
            parser.print_help()
    
    except Exception as e:
        print(f"âŒ Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
